# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
pip install mtcnn # for bounding box of face in a image
import numpy as np # linear algebra
import pandas as pd
import os
import cv2
from mtcnn.mtcnn import MTCNN
detector = MTCNN()
def cropFaces(img):
    
    # detect faces in the image
    faces = detector.detect_faces(img)
    #print(faces)
    if len(faces) == 0:
        return faces
    x, y, width, height = faces[0]['box']
    #img[y:y+height , x:x+width, :]
    return img[y:y+height, x:x+width, :]
    from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
label_encoder=preprocessing.LabelEncoder()
images_train = []
label_train = []
images_test = []
label_test = []
for dirname, _, filenames in os.walk('/kaggle/input/datanew/data'): #Add file path here
    for filename in filenames:
        
        img = cv2.imread(os.path.join(dirname,filename))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cropFaces(img)
        if(len(img) == 0 or img.shape[0] == 0 or img.shape[1] == 0 ):
            continue
        img = cv2.resize(img,(224,224))/255
        check = filename.split('_')[-1]
        lbl = dirname.split('/')[-1]
       
        if check.split('.')[0] == 'script':        
            images_train.append(img)
            label_train.append(lbl)
        else:
            images_test.append(img)
            label_test.append(lbl)
        

df = pd.DataFrame()


df['image'] = images_train
df['label'] = label_train
df['label']=label_encoder.fit_transform(df['label'])
df['label'].unique()
df = df.sample(frac = 1)
X_train=np.array(df.iloc[:,0].tolist())
y_train = np.array(df.iloc[:,1].tolist())


df_updated=df.drop(df['label'])
print(df_updated)
df2 = pd.DataFrame()   #we are creating this dataframe to shuffle the input dataset.


df2['image'] = images_test
df2['label'] = label_test
df2['label']=label_encoder.fit_transform(df2['label'])
df2['label'].unique()
df2 = df2.sample(frac = 1)
X_test=np.array(df2.iloc[:,0].tolist())
y_test = np.array(df2.iloc[:,1].tolist())


df2_updated=df2.drop(df2['label'])
# function for creating a block

def resnet(layer_in, n_filter,s):
    data = layer_in
    stride = (s,s)
    
    #1st Convolutional Layer
    merge_input = Conv2D(n_filter, (1,1),strides = (2,2))(layer_in)        
    bn2 = BatchNormalization(axis = 1,epsilon=2e-5,momentum =0.9)(merge_input)
    act2 = Activation('relu')(bn2)
    
    #2nd Convolutional Layer
    conv2 = Conv2D(n_filter, (3,3),strides= stride,use_bias = False, padding='same', kernel_initializer='he_normal')(act2)  
    bn3 = BatchNormalization(axis = 1,epsilon=2e-5,momentum =0.9)(conv2)
    act3 = Activation('relu')(bn3)
    
    #3rd Convolutional layer
    conv3 = Conv2D(n_filter, (1,1),use_bias = False, kernel_initializer='he_normal')(act3)  
    
    
    data = Conv2D(n_filter,(1,1),padding = 'valid',strides = (2,2))(data) # Adjusting the input size according to 3rd convolutional layer
    data = BatchNormalization()(data)  
    # add filters, assumes filters/channels last
    layer_out = Add()([conv3, data])
    layer_out = Activation('relu')(layer_out)
    return layer_out
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
model = Sequential([
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128,activation='relu')
    ])


    from keras.models import Model
from keras.layers import *
from keras.regularizers import *
from keras.optimizers import RMSprop

#define model input
visible = Input(shape=(224,224, 3))
layer1 = resnet(visible,64,1)
layer2 = resnet(layer1,128,1)
layer4 = resnet(layer2,256,1)
layer5 = resnet(layer4,256,1)
layer6 = resnet(layer5,512,1)
layer7 = resnet(layer6,512,1)
layer8 = resnet(layer7,1024,1)
layert = Dropout(0.5)(layer8)
layer9 = resnet(layert,2048,1)
layert2 = Dropout(0.5)(layer9)
layer10 = resnet(layert2,4096,1)
x = GlobalAveragePooling2D()(layer10)
x = Dropout(0.7)(x)
#flatten = Flatten()(layer9)
den = Dense(2048,activation = 'sigmoid')(x)
final = Dense(5, activation="softmax")(den)
#create model
model = Model(inputs=visible, outputs=final)

#compiling model
lr = 1e-5
decay = 1e-7 #0.0
optimizer = RMSprop(lr=lr, decay=decay)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
# summarize model
model.summary()
history = model.fit(X_train,y_train,steps_per_epoch = 20,shuffle = True,epochs = 65)
score = model.evaluate(X_test, y_test ,verbose=1)
print('Test Loss:', score[0])
print('Test accuracy:', score[1])
